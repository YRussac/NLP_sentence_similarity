{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_data = '../datasets'\n",
    "path_w2v = '/Volumes/DD_Supp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smart_stopwords.txt', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(path_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove dashes and apostrophes from punctuation marks\n",
    "punct = string.punctuation.replace('-', '').replace(\"'\",'')\n",
    "# regex to match intra-word dashes and intra-word apostrophes\n",
    "my_regex = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "\n",
    "def clean_string(string, punct=punct, my_regex=my_regex, to_lower=True, stpwds=None, minstr=3, maxstr=25):\n",
    "    if to_lower:\n",
    "        string = string.lower()\n",
    "    # remove formatting\n",
    "    str = re.sub('\\s+', ' ', string)\n",
    "     # remove punctuation\n",
    "    str = ''.join(l for l in str if l not in punct)\n",
    "    # remove dashes that are not intra-word\n",
    "    str = my_regex.sub(lambda x: (x.group(1) if x.group(1) else ' '), str)\n",
    "    # strip extra white space\n",
    "    str = re.sub(' +',' ',str)\n",
    "    # strip leading and trailing white space\n",
    "    str = str.strip()\n",
    "    # tokenize\n",
    "    tokens = str.split(' ')\n",
    "    # remove stopwords\n",
    "    if stpwds != None:\n",
    "        tokens = [token for token in tokens if token not in stpwds]\n",
    "    # remove digits\n",
    "    tokens = [''.join([elt for elt in token if not elt.isdigit()]) for token in tokens]\n",
    "    # remove tokens shorter than 3 characters in size\n",
    "    tokens = [token for token in tokens if len(token)>=minstr]\n",
    "    # remove tokens exceeding 25 characters in size\n",
    "    tokens = [token for token in tokens if len(token)<=maxstr]\n",
    "    return str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {}\n",
    "clean_texts = {}\n",
    "tokens = {}\n",
    "pairs_train = []\n",
    "pairs_test = []\n",
    "y_train = []\n",
    "\n",
    "with open(path_data+'/train.csv','r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = l[3]\n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = l[4]\n",
    "\n",
    "        pairs_train.append([l[1],l[2]])\n",
    "\n",
    "        y_train.append(int(l[5][:-1])) # [:-1] is just to remove formatting at the end\n",
    "    \n",
    "\n",
    "with open(path_data+'/test.csv','r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = l[3]\n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = l[4][:-1]\n",
    "\n",
    "        pairs_test.append([l[1], l[2]])\n",
    "        \n",
    "for key in list(texts.keys()):\n",
    "    clean_text = clean_string(texts[key])\n",
    "    clean_texts[key] = clean_text[0]\n",
    "    tokens[key] = clean_text[1]\n",
    "\n",
    "\n",
    "ids2ind = {} # will contain the row idx of each unique text in the TFIDF matrix\n",
    "for qid in texts:\n",
    "    ids2ind[qid] = len(ids2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "M = vec.fit_transform(texts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58940"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_q = 300 # to match dim of GNews word vectors\n",
    "mcount = 2\n",
    "w2v_perso = Word2Vec(size=my_q, min_count=mcount)\n",
    "w2v_perso.build_vocab(token) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
